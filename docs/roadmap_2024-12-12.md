# Peptide TRM Development Roadmap
**Date**: December 12, 2024
**Current Status**: Training aggressive noise curriculum to unlock multi-step refinement

---

## ğŸ” Current Issues & Lessons Learned

### Spectrum Loss Challenges

**Problem**: Spectrum matching loss stuck at 94-96% (only 4-6% coverage) despite 91% token accuracy.

**Root Cause**:
- Sigma parameter (0.2 Da) too narrow for current model accuracy
- With 9% token errors â†’ 50-200 Da fragment mass errors
- Gaussian kernel requires peaks within ~0.4-0.6 Da to match
- Essentially zero gradient signal for learning

**Decision**:
- **Disabled for now** (weight=0.0 in aggressive noise config)
- Not worth debugging on clean MS2PIP synthetic data
- Model achieves 91% accuracy with essentially NO spectrum signal
- May revisit when testing on real data with adaptive sigma curriculum

**Future Directions**:
1. Implement sigma curriculum: 5 Da â†’ 0.5 Da as accuracy improves
2. Test on real data where spectrum loss might help with:
   - Isomer disambiguation (same sequence, different fragments)
   - Incomplete sequences (N-term or C-term truncation)
   - Modified peptides (oxidation, phosphorylation, etc.)
3. Consider alternative formulations:
   - Wasserstein distance between predicted/observed spectra
   - Learned similarity metrics (neural network-based)
   - Multi-scale matching (coarse to fine)

---

## ğŸ”„ Recursion Improvements

### Current Plateau Issue

**Observation**: Model refines from step 0â†’1 (loss: 0.98 â†’ 0.83) but plateaus at steps 2-7 (all ~0.82)

**Diagnosis**:
- Linear iteration weighting: Step 7 only gets 22% of total loss (insufficient incentive)
- Data too clean: No noise until step 37.5K in old curriculum
- Model doesn't NEED recursion when data is easy
- Missing architectural features for multi-step refinement

**Current Solution** (In Progress):
- âœ… Exponential weighting: Step 7 now gets 50% of loss
- âœ… Aggressive noise curriculum: Forces recursion from step 0
- âœ… Clean/noisy mixing: 80% clean â†’ 0% clean over 45K steps

### Proposed Architecture Improvements

Based on `docs/recursion_update.txt` analysis:

#### 1. Step Embeddings â­â­â­ (High Priority)
```python
# Add to RecursiveCore.__init__:
self.step_embedding = nn.Embedding(num_supervision_steps + 1, hidden_dim)

# In forward loop:
step_emb = self.step_embedding(torch.tensor(step, device=device))
z = z + step_emb.unsqueeze(0).unsqueeze(0)
```

**Benefits**:
- Model learns different strategies per refinement step
- Rough draft (step 0) vs polishing (step 7)
- Proven in diffusion models, iterative refinement literature
- Very easy to implement (~5 lines of code)

**When to implement**: After aggressive noise run confirms recursion helps

---

#### 2. Residual Format for Answer Step â­â­â­ (Critical)
```python
# Current (WRONG):
y_logits = self.decoder.answer_step(...)  # Predicts full state

# Proposed (RIGHT):
y_delta = self.decoder.answer_step(...)   # Predicts change
y_logits = y_logits + y_delta              # Apply delta
```

**Current Status**:
- `latent_step`: âœ… Already has residual connection
- `answer_step`: âŒ No residual - overwrites instead of refines

**Benefits**:
- Forces model to learn refinements (deltas) not full states
- Prevents fixed-point convergence where updates become identity
- Standard in ResNet, Transformers, Neural ODEs

**Implementation Notes**:
- Medium complexity - requires careful handling of logits
- May need layer norm after residual addition
- Consider gating mechanism (learned weight on delta)

**When to implement**: After step embeddings prove successful

---

## ğŸŒ‰ Sim-to-Real Transition Strategies

### The Core Challenge
**Synthetic data (MS2PIP)**: Clean, predictable, perfect physics
**Real data**: Noisy, missing peaks, unexpected fragments, instrument artifacts

### Strategy 1: Mixed Sim/Real Curriculum â­â­â­ (Recommended First Approach)

**Concept**: Same as our current clean/noisy mixing, but with synthetic/real data

**Proposed Schedule**:
```
Stage 1 (0-20K):   100% synthetic â†’ Learn basic patterns
Stage 2 (20K-40K):  80% synthetic, 20% real â†’ Gentle exposure
Stage 3 (40K-60K):  60% synthetic, 40% real â†’ Increase real data
Stage 4 (60K-80K):  40% synthetic, 60% real â†’ Majority real
Stage 5 (80K-100K): 20% synthetic, 80% real â†’ Real-world focus
Stage 6 (100K+):     0% synthetic, 100% real â†’ Pure real data
```

**Advantages**:
- Proven technique in domain adaptation
- Model gets synthetic "scaffold" before facing real complexity
- Gradual transition reduces catastrophic forgetting
- Can adjust ratios based on real data quality/quantity

**Requirements**:
- Real MS/MS dataset with ground truth sequences (e.g., ProteomeTools, NIST, MassIVE-KB)
- Dataset class that mixes synthetic and real samples
- Validation on held-out real data only

**Implementation**:
```python
# Add to MS2PIPDataset:
self.real_data_ratio = 0.0  # 0.0=all synthetic, 1.0=all real

# In _generate_sample():
if np.random.rand() < self.real_data_ratio:
    return self._load_real_sample()
else:
    return self._generate_synthetic_sample()
```

---

### Strategy 2: Two-Stage Training (Pre-train + Fine-tune) â­â­

**Phase 1**: Pre-train on synthetic data to convergence
**Phase 2**: Fine-tune on real data with lower learning rate

**Advantages**:
- Simpler than mixed curriculum
- Can use much smaller real dataset for fine-tuning
- Clear separation of concerns

**Disadvantages**:
- Risk of overfitting to synthetic patterns
- Catastrophic forgetting when switching to real data
- May need to unfreeze layers gradually

**When to use**: If real dataset is small (<10K spectra)

---

### Strategy 3: Adversarial Domain Adaptation â­

**Concept**: Add discriminator that tries to distinguish synthetic vs real spectra
**Goal**: Encoder learns domain-invariant features

**Architecture**:
```python
# Add discriminator head on encoder output
domain_logits = discriminator(encoded_spectrum)  # Predict: synthetic or real?

# Train encoder to confuse discriminator (GAN-style)
domain_loss = -cross_entropy(domain_logits, domain_labels)  # Gradient reversal

# Total loss:
loss = ce_loss + precursor_loss + lambda_domain * domain_loss
```

**Advantages**:
- Model learns features that work on both domains
- Proven in computer vision (DANN, CycleGAN, etc.)

**Disadvantages**:
- More complex training (adversarial dynamics can be unstable)
- Requires careful hyperparameter tuning (lambda_domain)
- May need gradient reversal layer

**When to use**: If simple mixing doesn't work well enough

---

### Strategy 4: Noise Model Learning â­â­

**Concept**: Learn what real noise looks like, inject into synthetic data

**Approach**:
1. Collect real spectra with known sequences
2. Train noise model: `Real Spectrum = MS2PIP(sequence) + Learned Noise`
3. Use learned noise model to augment synthetic training data

**Advantages**:
- Makes synthetic data more realistic without needing full real dataset
- Can model instrument-specific noise patterns
- Interpretable (can visualize learned noise)

**Disadvantages**:
- Requires real data with ground truth
- Noise model may not capture all real-world complexity

**Implementation Ideas**:
- Variational autoencoder on residuals: `VAE(real_spectrum - synthetic_spectrum)`
- Conditional GAN: Generate realistic noise given sequence/spectrum
- Statistical noise models (mixture of Gaussians, peak dropout distributions)

---

### Strategy 5: Meta-Learning (MAML-style) â­ (Advanced)

**Concept**: Train to quickly adapt to new domains/instruments

**Approach**:
1. Pre-train on synthetic with explicit "adaptation" phase in inner loop
2. At deployment, fine-tune on small set of real spectra from target instrument
3. Model learns to learn â†’ fast adaptation to new domains

**When to use**: If deploying across many different instruments/labs

---

## ğŸ“Š Recommended Sim-to-Real Roadmap

### Phase 1: Validate on Synthetic Noise (Current)
- âœ… Train with aggressive noise curriculum
- âœ… Verify recursion unlocks with exponential weighting
- ğŸ”² Add step embeddings and residual format if needed
- **Goal**: Achieve >95% accuracy on noisy synthetic data

### Phase 2: Test on Real Data (Validation Only)
- ğŸ”² Download ProteomeTools or NIST real MS/MS dataset
- ğŸ”² Evaluate current model (trained on synthetic) on real data
- ğŸ”² Analyze failure modes:
  - Is it fragmentation patterns? (wrong peaks predicted)
  - Is it noise handling? (model confused by noise)
  - Is it missing peaks? (model expects peaks that aren't there)
- **Goal**: Understand sim-to-real gap

### Phase 3: Implement Mixed Curriculum
- ğŸ”² Create RealSpectrumDataset class (load from mzML/MGF files)
- ğŸ”² Implement sim/real mixing in dataset sampler
- ğŸ”² Train with 100% synthetic â†’ 100% real curriculum
- ğŸ”² Monitor validation on real data throughout training
- **Goal**: Achieve >90% accuracy on real data

### Phase 4: Advanced Techniques (If Needed)
- ğŸ”² If mixed curriculum insufficient: Try adversarial domain adaptation
- ğŸ”² If real data limited: Implement noise model learning
- ğŸ”² If instrument-specific: Consider meta-learning approach

---

## ğŸ’­ Open Questions

### Recursion
1. Will noise alone unlock multi-step refinement? (Testing now)
2. Do we need step embeddings, or will exponential weighting suffice?
3. Should we increase num_latent_steps (currently 6) for harder data?

### Spectrum Loss
1. Is spectrum loss useful on real data where chemistry isn't perfect?
2. What's the right sigma curriculum? Start at 10 Da or 5 Da?
3. Should we use different sigmas for different ion types (b vs y vs b++)?

### Sim-to-Real
1. How much real data is enough? (10K? 50K? 100K spectra?)
2. Should we fine-tune all parameters or freeze encoder?
3. Do we need instrument-specific models, or can one model handle all?
4. How to handle modified peptides in real data? (PTMs not in synthetic)

### Architecture
1. Is 384 hidden_dim enough for real-world complexity?
2. Should we add more encoder layers to handle noise better?
3. Would a hierarchical decoder (coarse-to-fine) help with long sequences?

---

## ğŸ¯ Success Metrics

### Synthetic Data (MS2PIP)
- âœ… >90% token accuracy on clean data (achieved: 91.3%)
- ğŸ”² >85% token accuracy on 45% dropout, 30 noise peaks
- ğŸ”² Clear refinement across all 8 steps (not just step 0â†’1)
- ğŸ”² Graceful degradation with increasing noise

### Real Data (Target)
- ğŸ”² >90% token accuracy on ProteomeTools benchmark
- ğŸ”² >85% token accuracy on NIST library
- ğŸ”² >80% token accuracy on "in-the-wild" MassIVE data
- ğŸ”² Competitive with or better than Casanovo/DeepNovo

### Generalization
- ğŸ”² Works across different instruments (Orbitrap, Q-TOF, etc.)
- ğŸ”² Handles peptides 7-30 amino acids
- ğŸ”² Robust to charge states 2-4
- ğŸ”² Graceful failure on modified peptides (identify as uncertain)

---

## ğŸ“š References & Resources

### Real MS/MS Datasets
- **ProteomeTools**: 1.4M synthetic peptides, high-quality HCD spectra
- **NIST Library**: Curated, high-confidence identifications
- **MassIVE-KB**: Large-scale, diverse real-world data
- **PeptideAtlas**: Tissue-specific peptide identifications

### Related Work
- **Casanovo**: Current SOTA, transformer-based, trained on real data
- **DeepNovo**: Original deep learning approach, LSTM-based
- **PointNovo**: Graph-based, handles PTMs
- **InstaNovo**: Fast inference, works on limited data

### Architecture Inspirations
- **Diffusion Models**: Step embeddings, iterative refinement
- **Neural ODEs**: Continuous-depth residual connections
- **DETR**: Transformer for structured prediction
- **AlphaFold**: Recycling iterations (similar to our recursion)

---

## ğŸ”§ Implementation Priorities

### Immediate (This Week)
1. âœ… Monitor aggressive noise training run
2. ğŸ”² Analyze per-step CE losses: Are ALL steps improving?
3. ğŸ”² Check refinement tracker: How many edits per step?

### Short-term (Next 2 Weeks)
1. ğŸ”² Implement step embeddings (if recursion unlocked)
2. ğŸ”² Implement residual format for answer_step
3. ğŸ”² Download and preprocess real MS/MS dataset
4. ğŸ”² Evaluate current model on real data (validation only)

### Medium-term (Next Month)
1. ğŸ”² Implement RealSpectrumDataset class
2. ğŸ”² Train with mixed sim/real curriculum
3. ğŸ”² Benchmark against Casanovo on standard test sets
4. ğŸ”² Write up results: "Does recursion help for noisy de novo sequencing?"

### Long-term (Next Quarter)
1. ğŸ”² Optimize for production: quantization, pruning, distillation
2. ğŸ”² Handle modified peptides (PTM prediction)
3. ğŸ”² Multi-instrument generalization
4. ğŸ”² Uncertainty quantification (when is model confident?)

---

**Last Updated**: 2024-12-12
**Next Review**: After aggressive noise training completes (~24 hours)
