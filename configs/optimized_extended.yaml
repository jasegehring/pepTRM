# Optimized configuration for RTX 4090 with extended, smoother curriculum
# Expected training time: ~2.5 hours for 100K steps (vs ~7 hours without optimizations)

model:
  hidden_dim: 384  # INCREASED from 256 (1.5x capacity)
  num_encoder_layers: 3  # INCREASED from 2 (more expressiveness)
  num_decoder_layers: 3  # INCREASED from 2 (more expressiveness)
  num_heads: 6  # INCREASED from 4 (384/6 = 64 per head)
  max_peaks: 100
  max_seq_len: 35  # Supports peptides up to 30aa (with SOS/EOS/padding)
  num_supervision_steps: 8
  num_latent_steps: 6  # INCREASED from 4 (original TRM value)
  dropout: 0.1

training:
  # Optimization (3x faster!)
  learning_rate: 1.5e-4  # Tuned for batch=96
  weight_decay: 0.01
  batch_size: 80  # torch.compile needs headroom for CUDA graphs
  max_steps: 100000  # 2x longer training
  warmup_steps: 2000  # Proportional increase

  # Mixed Precision (NEW - 2-3x speedup)
  use_amp: true
  amp_dtype: 'bfloat16'  # Use 'bfloat16' if available

  # Gradient Accumulation - DISABLED (not needed with batch=80)
  gradient_accumulation_steps: 1  # No accumulation - direct batch of 80

  # Model Compilation - TESTING with reduced batch size
  use_compile: true  # TESTING: Lower batch might avoid OOM in CUDA graph capture
  compile_mode: 'reduce-overhead'  # Best for recurrent models

  # Loss configuration
  ce_weight: 1.0
  spectrum_weight: 0.0  # Controlled by curriculum
  iteration_weights: 'linear'
  label_smoothing: 0.1  # Increased for robustness

  # Curriculum (smoother, extended)
  use_curriculum: true

  # EMA (critical for stability)
  use_ema: true
  ema_decay: 0.999

  # Logging (more frequent for longer training)
  log_interval: 100
  eval_interval: 1000
  save_interval: 5000

  # Paths
  checkpoint_dir: 'checkpoints_optimized'

  # Device (auto-detected)
  device: 'cuda'

data:
  # MS2PIP-specific settings
  ms2pip_model: 'HCDch2'  # UPDATED: Use HCDch2 for b, y, b++, y++ ions
  top_k_peaks: 50

  # Charge distribution
  charge_distribution:
    2: 0.7
    3: 0.3

# Real data validation - track performance on real data while training on synthetic
# This helps detect if we're over-optimizing for simulation artifacts
real_data_validation:
  enabled: true
  eval_interval: 2000  # Evaluate every N steps (less frequent than synthetic validation)

  # ProteomeTools: Synthetic peptide library (high-quality, comprehensive)
  proteometools:
    enabled: true
    data_dir: 'data/proteometools'
    batch_size: 64
    num_batches: 20  # Number of batches to evaluate (quick validation)
    max_samples: 5000  # Only load 5K samples for fast validation

  # Nine-Species: Real biological samples (9 species benchmark)
  nine_species:
    enabled: true
    data_dir: 'data/nine_species'
    batch_size: 64
    num_batches: 20
    max_samples: 5000  # Only load 5K samples for fast validation
    use_balanced: true  # Use balanced version (780K PSMs, ~13GB extracted)
