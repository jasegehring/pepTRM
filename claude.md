# PepTRM: Recursive Transformer for De Novo Peptide Sequencing
**Status**: Active Development | **Latest**: Step embeddings + gated residuals implemented
**Last Updated**: December 13, 2024 (Architecture v2.1)

---

## Project Overview

**Goal**: Implement a Tiny Recursive Model (TRM) for de novo peptide sequencing from mass spectrometry (MS/MS) data, capable of handling noisy real-world spectra through iterative refinement.

**Core Insight**: Unlike standard autoregressive models that generate sequences in a single pass, this model uses a recursive loop to iteratively refine predictions. Mass spectrometry provides hard physical constraints (fragment masses must match observed peaks), making this domain uniquely suited for iterative refinementâ€”the model can learn to "check its work" against mass constraints and self-correct.

**Key Innovations**:
1. **Physics-aware losses**: Spectrum matching + precursor mass constraints
2. **Iterative refinement**: 8 supervision steps with 6 latent reasoning steps each
3. **Curriculum learning**: Progressive noise introduction for robustness
4. **Real data integration**: Seamless synthetic â†’ real data transition

**Reference**: "Less is More: Recursive Reasoning with Tiny Networks" (Jolicoeur-Martineau, 2025)

---

## Architecture Overview

### Core TRM Loop (v2.1 - with Step Embeddings & Gated Residuals)

```
Initialize: yâ‚€ (sequence logits), zâ‚€ (latent state)

For each supervision step t âˆˆ {0, ..., 7}:
    step_emb = StepEmbedding(t)     # NEW: Model knows which iteration it's on

    For n=6 latent reasoning steps:
        z = LayerNorm(z + TransformerBlock(z + step_emb, spectrum_encoding))  # Think

    # NEW: GRU-style gated residual (not just OutputHead)
    candidate = OutputHead(z + step_emb)
    gate = Sigmoid(GateHead(z + step_emb))  # gate âˆˆ [0,1], initialized conservative
    y = (1 - gate) * y_prev + gate * candidate  # Interpolate prev and new

    Loss_t = CrossEntropy(y, target) + Î»_precursor * PrecursorLoss(y, precursor_mass)

Total Loss = Î£_t w_t * Loss_t     # Deep supervision with exponential weighting
```

**Key v2.1 Improvements**:
- **Step embeddings**: Model knows "am I on step 0 (rough draft) or step 7 (polishing)?"
- **Gated residuals**: Model decides per-position how much to update vs keep previous
- **Conservative init**: Gate bias = -2.0, so initial gate â‰ˆ 12% (must "earn" right to change)

**Key Components**:
- **Spectrum Encoding** (x): MS/MS peaks (m/z + intensities) encoded via Transformer
- **Sequence Hypothesis** (y): Predicted amino acid sequence (logits over vocabulary)
- **Latent State** (z): Hidden "scratchpad" for multi-step reasoning
- **Supervision Steps**: 8 (allows 8 attempts to refine prediction)
- **Latent Steps**: 6 per supervision (gives model time to "think")

### Model Architecture

```
Input Spectrum (100 peaks Ã— 2 features)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spectrum Encoder               â”‚
â”‚  - Peak Embeddings (mass + int) â”‚
â”‚  - 3-layer Transformer          â”‚
â”‚  - 6 attention heads            â”‚
â”‚  Hidden dim: 384                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (encoded spectrum)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Recursive Decoder Core         â”‚
â”‚  - Shared weights (reused 8Ã—)   â”‚
â”‚  - 3-layer Transformer          â”‚
â”‚  - Cross-attention to spectrum  â”‚
â”‚  - Soft embeddings from probs   â”‚
â”‚  - Residual connections         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (8 refinement iterations)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deep Supervision               â”‚
â”‚  - CE loss at ALL 8 steps       â”‚
â”‚  - Exponential weighting        â”‚
â”‚  - Precursor mass constraint    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output: Refined sequence prediction
```

**Model Size**: 12.5M parameters
**Training Speed**: ~4.4 it/s with torch.compile (RTX 4090)

---

## Current Status (December 2024)

### âœ… **Achievements**

**Infrastructure**:
- âœ… torch.compile working (2.1x speedup with batch_size=80)
- âœ… Mixed precision training (bfloat16)
- âœ… Gradient accumulation for memory efficiency
- âœ… EMA for training stability
- âœ… W&B integration for experiment tracking

**Data Pipeline**:
- âœ… MS2PIP synthetic data (infinite, configurable noise)
- âœ… ProteomeTools dataset loader (21M high-quality synthetic spectra)
- âœ… Nine-Species dataset loader (2.8M real PSMs from 9 species)
- âœ… Identical interface for all datasets (easy switching)
- âœ… Curriculum learning with dynamic noise adjustment

**Model Performance**:
- âœ… **91.3% token accuracy** on clean synthetic data
- âœ… **58% sequence accuracy** on clean data
- âœ… Physics tests: 100% passing
- âœ… Training stable with EMA

### ğŸ”„ **In Progress**

**Architecture v2.1** (Step Embeddings + Gated Residuals) - JUST IMPLEMENTED:
- âœ… Step embeddings: Model knows iteration index (0-7)
- âœ… GRU-style gated residuals in answer_step
- âœ… Gate initialized conservative (bias=-2.0, ~12% initial activation)
- âœ… Local metrics logging (logs/metrics_*.jsonl)
- âœ… Analysis tool (tools/analyze_runs.py)
- **Next**: Train and validate that recursion plateau is fixed
- **Goal**: See progressive improvement across ALL 8 steps

**Previous Run** (Aggressive Noise Curriculum at 57K steps):
- Recursion plateau confirmed (steps 1-6 all similar loss)
- val_hard at 90% while training at 30% (curriculum harder than validation)
- ProteomeTools: 11% token accuracy (better than random but struggling)

### âŒ **Known Challenges**

1. **Recursion Plateau** ğŸŸ¡ **FIX IMPLEMENTED - NEEDS TESTING**
   - Model refines step 0â†’1 (loss: 0.98 â†’ 0.83) âœ…
   - But plateaus steps 2-7 (all ~0.82 loss) âŒ
   - Only using ~2 of 8 available refinement steps
   - **Fix implemented**: Step embeddings + GRU-style gated residuals (v2.1)
   - **Next**: Train and validate the fix works

2. **Spectrum Loss Challenges**
   - Stuck at 94-96% with sigma=0.2 Da (only 4-6% coverage)
   - Root cause: Sigma too narrow for current model accuracy
   - With 9% token errors â†’ 50-200 Da fragment mass errors
   - **Solution**: Disabled for now, testing precursor loss only

3. **Sim-to-Real Gap** (Unknown)
   - Haven't tested on real data yet
   - Expect lower accuracy (60-75%) on Nine-Species benchmark

---

## Key Design Decisions

### **1. Deep Supervision with Exponential Weighting**

**Why**: Force model to optimize ALL refinement steps, not just early ones

**Iteration Weights**:
- Linear (old): [2.8%, 5.6%, 8.3%, ..., 22%] â†’ step 7 only gets 22% of gradient
- **Exponential (new)**: [0.4%, 0.8%, 1.6%, ..., 50%] â†’ step 7 gets 50% of gradient

**Impact**: Should unlock multi-step refinement

### **2. Curriculum Learning (Clean/Noisy Mixing)**

**Philosophy**: Start with mostly clean data, gradually increase noise proportion

**Schedule**:
```
Stage 1 (0-10K):   80% clean, 20% noisy â†’ Learn basics
Stage 2 (10K-20K): 60% clean, 40% noisy â†’ More challenge
Stage 3 (20K-30K): 40% clean, 60% noisy â†’ Majority noisy
Stage 4 (30K-45K): 20% clean, 80% noisy â†’ Force robustness
Stage 5 (45K+):     0% clean, 100% noisy â†’ Pure realistic data
```

**Noise Profile** (Real-world MS/MS):
- 5-30 random noise peaks
- 15-45% peak dropout (missing fragments)
- 5-20 ppm mass error
- 20-70% intensity variation

### **3. Physics-Based Losses**

**Precursor Mass Loss** (Enabled):
```python
# Normalize mass error by average AA mass (110 Da)
# Balances gradient magnitudes with CrossEntropy loss
predicted_mass = Î£ P(aa_i) * mass(aa_i) + H2O_mass
error_da = predicted_mass - observed_precursor_mass
loss = SmoothL1(error_da / 110.0)  # Normalized to "AA units"
```

**Spectrum Matching Loss** (Disabled for now):
- Current: Gaussian kernel with sigma=0.2 Da
- Problem: Too narrow for 9% token error rate
- Future: Adaptive sigma curriculum (10 Da â†’ 0.5 Da)
- Alternative: Matched filter loss (recall-only, robust to dropout)

### **4. EMA (Exponential Moving Average)**

**Critical for stability**:
- Decay: 0.999
- Always use EMA model for validation/inference
- TRM paper reports instability without EMA

### **5. Soft Embeddings**

**Enable gradient flow through recursive loop**:
```python
# Convert probability distribution to continuous embedding
probs = softmax(logits)  # (batch, seq_len, vocab_size)
soft_embedding = learned_projection(probs)  # Differentiable
```

**Why**: Allows model to receive gradient signal about "almost correct" predictions

---

## Domain Knowledge: Mass Spectrometry

### Physical Constraints

**Peptide Mass**:
```
Precursor mass = Î£(amino acid masses) + H2O_mass
```

**Fragment Ions**:
- **b-ions**: N-terminal fragments (mass = cumsum from left)
- **y-ions**: C-terminal fragments (mass = cumsum from right + H2O)
- **b++, y++**: Doubly-charged fragments (common in charge 2+ precursors)
  - Formula: (neutral_mass + 2 Ã— proton_mass) / 2
  - **Status**: Not yet implemented (critical missing physics)

**Key Constants** (from UniProt):
- Water mass: 18.01056 Da
- Proton mass: 1.00727 Da
- Average AA mass: ~110 Da

**Tolerances**:
- High-resolution MS: 5-20 ppm (0.01-0.04 Da at m/z 2000)
- Low-resolution MS: 50-100 ppm

### Isobaric Ambiguities

**Unresolvable by mass alone**:
- **I/L** (Isoleucine/Leucine): Identical mass (113.08406 Da)
- **K/Q** (Lysine/Glutamine): Very similar (Î” = 0.036 Da, within instrument tolerance)

**Current handling**: Model must learn from context (future: uncertainty quantification)

---

## Development Guidelines

### Quick Start

**1. Install Dependencies**:
```bash
pip install torch wandb omegaconf ms2pip
```

**2. Run Physics Tests** (Must pass 100%):
```bash
pytest tests/test_physics.py -v
```

**3. Quick Validation** (Overfit test):
```bash
python scripts/overfit_test.py
```
Target: >80% token accuracy after 500 steps on 1 batch

**4. Full Training**:
```bash
# Current aggressive noise run
python scripts/train_aggressive_noise.py

# Or standard optimized training
python scripts/train_optimized.py
```

### Configuration

**Main config**: `configs/aggressive_noise_test.yaml`

```yaml
model:
  hidden_dim: 384
  num_encoder_layers: 3
  num_decoder_layers: 3
  num_heads: 6
  num_supervision_steps: 8
  num_latent_steps: 6

training:
  batch_size: 80
  learning_rate: 1.5e-4
  use_amp: true
  use_compile: true
  iteration_weights: 'exponential'  # Force late-step optimization
  spectrum_weight: 0.0  # Disabled (sigma too narrow)

  # Curriculum
  use_curriculum: true
```

### Key Files

**Core Implementation**:
```
src/
â”œâ”€â”€ constants.py              # Amino acid masses (physics foundation)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ms2pip_dataset.py    # MS2PIP synthetic data
â”‚   â”œâ”€â”€ proteometools_dataset.py  # High-quality synthetic
â”‚   â””â”€â”€ nine_species_dataset.py   # Real biological data
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ encoder.py           # Spectrum encoder
â”‚   â”œâ”€â”€ decoder.py           # Recursive core (TRM innovation)
â”‚   â””â”€â”€ trm.py              # Full model
â””â”€â”€ training/
    â”œâ”€â”€ losses.py            # Deep supervision + physics losses
    â”œâ”€â”€ trainer_optimized.py # Optimized trainer (compile, AMP, etc.)
    â””â”€â”€ curriculum_aggressive_noise.py  # Current curriculum
```

**Entry Points**:
```
scripts/
â”œâ”€â”€ train_aggressive_noise.py  # Current experiment
â”œâ”€â”€ train_optimized.py         # Standard optimized training
â””â”€â”€ overfit_test.py           # Validation test
```

**Analysis Tools**:
```
tools/
â””â”€â”€ analyze_runs.py           # Parse wandb & local logs
                              # --live, --run <ID>, --local, --compare
logs/
â””â”€â”€ metrics_*.jsonl           # Local metrics (every 1000 steps)
```

**Documentation**:
```
docs/
â”œâ”€â”€ MASTER_ROADMAP_2024-12-12.md  # Complete development plan
â”œâ”€â”€ README_DOCUMENTATION.md        # Documentation index
â”œâ”€â”€ training_with_real_data.md    # Real data guide
â”œâ”€â”€ proteometools_dataset.md      # ProteomeTools setup
â””â”€â”€ nine_species_dataset.md       # Nine-Species benchmark
```

---

## Roadmap & Future Work

**See `docs/MASTER_ROADMAP_2024-12-12.md` for complete details.**

### **Phase 1: Unlock Multi-Step Recursion** ğŸ”´ **CURRENT**

**Goal**: Prove model can use all 8 refinement steps + achieve length generalization

**Current Experiment** (Dec 13):
- â³ Two-tier noise curriculum (`curriculum_complex_noise.py`)
  - Flat length 7-30 from step 0 (no progressive length)
  - "Grass" noise (low-intensity background) + "Spikes" (high-intensity decoys)
  - Signal suppression (crush real peaks to break intensity bias)
  - Pure foundation phase (5K steps, 100% clean, all lengths)
- â³ DataLoader iterator bug FIXED (workers now see curriculum updates)
- â³ Monitor: length-bucketed accuracy, val_hard, real data transfer

**Previous Run Learnings** (36K steps):
- âœ… Bug fix working (better val_hard performance)
- âš ï¸ Short peptides plateau early (positional overfitting)
- âš ï¸ Long peptides struggle (introduced too late)
- ğŸ“ˆ ProteomeTools reached 12% token acc (better than before!)

**Architecture v2.1 (Just Implemented)**:
- âœ… Step embeddings (model knows iteration 0-7)
- âœ… GRU-style gated residuals (conservative gate init, ~12% activation)
- ğŸ”² Train and validate recursion plateau is fixed
- ğŸ”² Integrate refinement tracker (monitor sequence changes per step)

### **Phase 1.5: Architecture Experiments** (After curriculum validated)

**Experiment: Perception-Heavy Architecture**
```yaml
# Hypothesis: Bigger encoder for noisy spectra, smaller decoder for fast iteration
hidden_dim: 384        # Keep
num_heads: 6           # Keep
encoder_layers: 6      # INCREASE (was 3) - Better noise parsing
decoder_layers: 2      # DECREASE (was 3) - Faster recursive loop
supervision_steps: 8   # Keep
latent_steps: 4        # DECREASE (was 6) - Peptide logic may need less
```
- Rationale: Spectrum parsing is hard (150+ noise peaks), sequence logic is constrained by mass
- Tradeoff: ~15M params, 32 thinking steps (vs 12.5M, 48 steps)
- Test AFTER proving curriculum works

**Experiment: Tiny TRM Challenge**
- Can we match Casanovo (47M) with <5M params?
- Slim config: hidden=256, enc=2, dec=2 â†’ 3.7M params
- Test AFTER architecture experiments

### **Phase 2: Advanced Architecture Features**

**Mass Gap Token** (for unknown modifications):
- Add `[GAP]` token to vocabulary
- Predict mass shift per position: delta_mass_head(z) â†’ Â±500 Da
- Compute fragment masses: base_mass + predicted_delta
- **Use case**: PTMs, non-standard AAs, open search

**Residual Spectrum Embedding** (physics feedback):
- Render observed vs predicted spectra
- Compute residual: what did we miss?
- Feed through CNN to detect "shift patterns"
- Inject into latent state for refinement
- **Use case**: Modified peptides, mass shift detection

### **Phase 3: Spectrum Loss Improvements** (Week 5)

**Matched Filter Loss** (recall-only, robust to dropout):
- Trust observed peaks (they're real)
- Don't trust absence (peaks may have dropped out)
- Let precursor loss handle precision (prevent hallucinations)

**Adaptive Sigma Curriculum**:
- Start wide: 10 Da (forgiving, learn rough patterns)
- Narrow over time: 10 â†’ 5 â†’ 2 â†’ 0.5 Da
- Match sigma to model accuracy

### **Phase 4: Real Data Transition** (Weeks 6-8) â­

**Mixed Synthetic/Real Curriculum** (Recommended):
```
100% synthetic â†’ 80/20 â†’ 50/50 â†’ 20/80 â†’ 100% real
Over 60-80K steps
```

**Benchmarks**:
- ğŸ”² ProteomeTools pre-training (21M spectra, synthetic)
- ğŸ”² Nine-Species 9-fold CV (2.8M PSMs, real data)
- ğŸ”² Compare vs Casanovo baseline

**Expected Performance**:
- ProteomeTools: 85-90% token accuracy
- Nine-Species: 60-75% token accuracy (real data is harder)

### **Phase 5: Missing Physics & Advanced Features** (Weeks 9-12)

**Critical Missing Physics**:
- ğŸ”² Doubly-charged ions (b++, y++) - common in real data
- ğŸ”² Common PTMs (oxidation, phosphorylation, acetylation)

**Production Features**:
- ğŸ”² Uncertainty quantification (entropy-based confidence)
- ğŸ”² Beam search inference
- ğŸ”² I/L ambiguity notation

---

## Performance Targets

### **Synthetic Data**

| Dataset | Token Acc | Sequence Acc | Notes |
|---------|-----------|--------------|-------|
| MS2PIP (clean) | **>90%** | **>60%** | Current: 91.3% / 58% âœ… |
| MS2PIP (noisy) | >75% | >30% | 45% dropout, 30 noise peaks |
| ProteomeTools | >85% | >50% | High-quality synthetic |

### **Real Data**

| Dataset | Token Acc | Sequence Acc | Notes |
|---------|-----------|--------------|-------|
| Nine-Species | >70% | >30% | Averaged across 9 folds |
| Nine-Species (easy) | >80% | >50% | Clean subset |

### **Ablation Studies** (Planned)

| Configuration | Expected Impact |
|---------------|-----------------|
| T=8 vs T=1 | Recursion adds +10-15% accuracy |
| With vs without spectrum loss | +5-10% (when sigma is right) |
| With vs without curriculum | Faster convergence, better final accuracy |
| Exponential vs linear weighting | Unlocks multi-step refinement |

---

## Important Constraints & Notes

### **Physics Validation**
- **All mass calculations must be validated** against UniProt reference values
- Tests in `tests/test_physics.py` are non-negotiable
- Wrong masses = meaningless results

### **EMA is Critical**
- Training unstable without EMA (from TRM paper)
- Use decay=0.999
- Always use EMA model for validation/inference

### **Recursion Debugging**
When recursion isn't working:
1. Check iteration weighting (exponential > linear)
2. Verify noise is challenging enough (easy data â†’ no need to refine)
3. Monitor per-step losses (should see gradient: step_0 > step_7)
4. Check refinement tracker (edit_rate per step)

### **Curriculum is Essential**
- Don't skip curriculum stages
- Model needs easy examples first
- Catastrophic forgetting happens with distribution shifts (0 noise â†’ 5 noise peaks)
- Use gradual transitions (0 â†’ 1 â†’ 2 â†’ 5 noise peaks)

### **I/L Ambiguity**
- Isoleucine and Leucine are **indistinguishable** by mass alone
- Model should learn to output one consistently (or indicate uncertainty)
- Future: Explicit uncertainty quantification

---

## Monitoring & Debugging

### **Key W&B Metrics**

**Per-step losses** (recursion diagnostics):
```python
ce_step_0: 0.98  # Initial rough guess
ce_step_1: 0.83  # First refinement (working! âœ…)
ce_step_2: 0.82  # Plateau starts (problem âŒ)
ce_step_7: 0.82  # Should be << ce_step_0 if recursion works
```

**Refinement tracker** (when integrated):
```python
edit_rate_step_{t}:      # % of tokens changed at step t
accuracy_step_{t}:       # % correct at step t
improvement_step_{t}:    # Net improvement from edits
```

**Loss breakdown**:
```python
train/ce_loss:          # Cross-entropy
train/precursor_loss:   # Precursor mass constraint
train/spectrum_loss:    # Spectrum matching (disabled for now)
```

### **Common Issues**

**"Model not refining past step 1"**:
- âœ… Switch to exponential weighting
- âœ… Increase noise difficulty (force model to need recursion)
- âœ… Add step embeddings (model knows iteration 0-7)
- âœ… GRU-style gated residuals (conservative init, ~12% gate activation)

**"Spectrum loss stuck at 95%"**:
- âœ… Disable for now (sigma too narrow)
- ğŸ”² Implement adaptive sigma curriculum
- ğŸ”² Try matched filter loss (recall-only)

**"Poor accuracy on real data"**:
- ğŸ”² Pre-train on ProteomeTools first
- ğŸ”² Use mixed synthetic/real curriculum
- ğŸ”² Add doubly-charged ions (critical missing physics)
- ğŸ”² Fine-tune with lower learning rate

---

## Code Quality Standards

### **When Making Changes**

1. **Run physics tests first**: `pytest tests/test_physics.py -v`
2. **Test on overfit scenario**: Can it memorize 1 batch?
3. **Update tests** if changing mass calculations or data generation
4. **Document domain-specific logic** (why EMA, why certain losses)
5. **Maintain type hints** throughout codebase

### **Architecture Principles**

- **Modularity**: Each component (encoder, decoder, losses) is independent
- **Configuration-driven**: No hardcoded hyperparameters
- **Physics-first**: Always validate against ground truth
- **Deep supervision**: Loss at ALL intermediate steps
- **Curriculum learning**: Progressive difficulty essential

---

## Resources

### **Documentation**
- **MASTER_ROADMAP_2024-12-12.md**: Complete development plan with priorities
- **README_DOCUMENTATION.md**: Documentation index
- **training_with_real_data.md**: Real data pipeline guide
- **memory_management.md**: GPU optimization strategies

### **Papers**
- TRM: Jolicoeur-Martineau et al., 2025
- ProteomeTools: Gessulat et al., Nature Methods, 2019
- Nine-Species: Eloff et al., Scientific Data, 2024

### **Datasets**
- MS2PIP: On-the-fly synthetic (unlimited)
- ProteomeTools: 21M synthetic spectra (2.3 GB download)
- Nine-Species: 2.8M real PSMs (15-50 GB download)

---

## License

MIT - See LICENSE file

---

**Quick Links**:
- ğŸ“Š [Master Roadmap](docs/MASTER_ROADMAP_2024-12-12.md)
- ğŸ“š [Documentation Index](docs/README_DOCUMENTATION.md)
- ğŸ”¬ [Training Guide](docs/training_with_real_data.md)
- âš™ï¸ [Memory Optimization](docs/memory_management.md)

**Status**: Active development, architecture v2.1 ready for training
**Next Milestone**: Validate step embeddings + gated residuals fix recursion plateau
